---
title: "Ensemble Principal Component Analysis"
collection: publications
permalink: /publication/2023-01-01-EPCA
excerpt: "We propose a scalable method called Ensemble PCA (EPCA) as a noise-resistant extension of PCA that lends itself naturally to uncertainty quantification.<br/><img src='/images/EnsemblePCAGraphic.pdf'><br/>Given a data matrix with inherently low-rank structure, we sample B bags of size n at random with replacement. We run PCA and store d principal components and their corresponding eigenvalues for each bag. We stack all components, along with their reflections to account for rotational variability. We also stack all eigenvalues, in accordance with the order in the matrix storing components. Next, we run k-means clustering on the stacked component matrix with 2d clusters."
date: 2023-01-01
venue: 'IEEE Access'
paperurl: 'http://OlgaD400.github.io/files/EPCA.pdf'
---
Efficient representations of data are essential for processing, exploration, and human under- standing, and Principal Component Analysis (PCA) is one of the most common dimensionality reduction techniques used for the analysis of large, multivariate datasets today. Two well-known limitations of the method include sensitivity to outliers and noise and no clear methodology for the uncertainty quantification of the principal components or their associated explained variances. Whereas previous work has focused on each of these problems individually, we propose a scalable method called Ensemble PCA (EPCA) that addresses them simultaneously for data which has an inherently low-rank structure. EPCA combines boostrapped PCA with k-means cluster analysis to handle challenges associated with sign-ambiguity and the re-ordering of components in the PCA subsamples. EPCA provides a noise-resistant extension of PCA that lends itself naturally to uncertainty quantification. We test EPCA on data corrupted with white noise, sparse noise, and outliers against both classical PCA and Robust PCA (RPCA) and show that EPCA performs competitively across different noise scenarios, with a clear advantage on datasets containing outliers and orders of magnitude reduction in computational cost compared to RPCA.<br/><img src='/images/EnsemblePCAGraphic.pdf'><br/>Given a data matrix with inherently low-rank structure, we sample B bags of size n at random with replacement. We run PCA and store d principal components and their corresponding eigenvalues for each bag. We stack all components, along with their reflections to account for rotational variability. We also stack all eigenvalues, in accordance with the order in the matrix storing components. Next, we run k-means clustering on the stacked component matrix with 2d clusters.

[Download paper here]([http://academicpages.github.io/files/paper2.pdf](http://OlgaD400.github.io/files/EPCA.pdf))
